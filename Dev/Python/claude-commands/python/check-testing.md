# V√©rification Tests Python

## Arguments

$ARGUMENTS (optionnel : chemin vers le projet √† analyser)

## MISSION

R√©aliser un audit complet de la strat√©gie de test du projet Python en v√©rifiant la couverture, la qualit√© des tests, et le respect des bonnes pratiques d√©finies dans les r√®gles du projet.

### √âtape 1 : Structure et organisation des tests

Examiner l'organisation des tests :
- [ ] Dossier `tests/` √† la racine du projet
- [ ] Structure miroir du code source (tests/domain, tests/application, etc.)
- [ ] Fichiers de test nomm√©s `test_*.py` ou `*_test.py`
- [ ] Fixtures pytest dans `conftest.py`
- [ ] S√©paration tests unitaires / int√©gration / e2e

**R√©f√©rence** : `rules/07-testing.md` section "Test Organization"

### √âtape 2 : Couverture de code (Coverage)

Mesurer la couverture des tests :
- [ ] Couverture globale ‚â• 80%
- [ ] Couverture Domain Layer ‚â• 90%
- [ ] Couverture Application Layer ‚â• 85%
- [ ] Fichiers critiques √† 100%
- [ ] Configuration coverage dans pyproject.toml

**Commande** : Ex√©cuter `docker run --rm -v $(pwd):/app python:3.11 sh -c "pip install pytest pytest-cov && pytest /app --cov=/app --cov-report=term-missing"`

**R√©f√©rence** : `rules/07-testing.md` section "Code Coverage"

### √âtape 3 : Tests unitaires

Analyser la qualit√© des tests unitaires :
- [ ] Tests isol√©s (pas de d√©pendances externes)
- [ ] Utilisation de mocks/stubs pour d√©pendances
- [ ] Tests rapides (<100ms par test)
- [ ] Un test = Un comportement
- [ ] Nommage descriptif : `test_should_X_when_Y`
- [ ] Pattern AAA (Arrange, Act, Assert)

**R√©f√©rence** : `rules/07-testing.md` section "Unit Tests"

### √âtape 4 : Tests d'int√©gration

V√©rifier les tests d'int√©gration :
- [ ] Tests des interactions entre composants
- [ ] Tests de la couche Infrastructure (DB, API, etc.)
- [ ] Utilisation de bases de donn√©es de test (fixtures)
- [ ] Nettoyage apr√®s chaque test (teardown)
- [ ] Tests isol√©s et ind√©pendants

**R√©f√©rence** : `rules/07-testing.md` section "Integration Tests"

### √âtape 5 : Assertions et qualit√© des tests

Contr√¥ler la qualit√© des assertions :
- [ ] Assertions explicites et sp√©cifiques
- [ ] Pas d'assertions multiples non li√©es
- [ ] Messages d'erreur clairs
- [ ] Tests des cas limites (edge cases)
- [ ] Tests des erreurs et exceptions
- [ ] Pas de tests d√©sactiv√©s sans raison (skip/xfail)

**R√©f√©rence** : `rules/07-testing.md` section "Assertions and Test Quality"

### √âtape 6 : Fixtures et param√©trage

√âvaluer l'utilisation des fixtures pytest :
- [ ] Fixtures pour setup/teardown communs
- [ ] Scope appropri√© (function, class, module, session)
- [ ] Param√©trage avec `@pytest.mark.parametrize`
- [ ] Factories pour objets de test complexes
- [ ] Pas de duplication dans les fixtures

**R√©f√©rence** : `rules/07-testing.md` section "Pytest Fixtures"

### √âtape 7 : Performance et ex√©cution

Analyser la performance des tests :
- [ ] Temps d'ex√©cution total <30 secondes (unitaires)
- [ ] Tests parall√©lisables (pytest-xdist)
- [ ] Pas de sleep() dans les tests
- [ ] Configuration pytest dans pyproject.toml
- [ ] CI/CD avec ex√©cution automatique des tests

**Commande** : Ex√©cuter `docker run --rm -v $(pwd):/app python:3.11 sh -c "pip install pytest && pytest /app -v --duration=10"`

**R√©f√©rence** : `rules/07-testing.md` section "Test Performance"

### √âtape 8 : Test-Driven Development (TDD)

V√©rifier l'adoption du TDD :
- [ ] Tests √©crits avant le code (si applicable)
- [ ] Red-Green-Refactor cycle
- [ ] Tests guidant le design
- [ ] Pas de code non test√© en production

**R√©f√©rence** : `rules/01-workflow-analysis.md` section "TDD Workflow"

### √âtape 9 : Calcul du score

Attribution des points (sur 25) :
- Couverture de code : 7 points
- Tests unitaires : 6 points
- Tests d'int√©gration : 4 points
- Qualit√© des assertions : 3 points
- Fixtures et organisation : 3 points
- Performance : 2 points

## FORMAT DE SORTIE

```
üß™ AUDIT TESTS PYTHON
================================

üìä SCORE GLOBAL : XX/25

‚úÖ POINTS FORTS :
- [Liste des bonnes pratiques de test observ√©es]

‚ö†Ô∏è POINTS D'AM√âLIORATION :
- [Liste des am√©liorations mineures]

‚ùå PROBL√àMES CRITIQUES :
- [Liste des manques critiques en tests]

üìã D√âTAILS PAR CAT√âGORIE :

1. COUVERTURE (XX/7)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Analyse de la couverture]
   Couverture globale : XX%
   Domain : XX%
   Application : XX%
   Infrastructure : XX%

2. TESTS UNITAIRES (XX/6)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Qualit√© des tests unitaires]
   Nombre de tests : XX
   Tests isol√©s : XX%
   Temps moyen : XXms

3. TESTS D'INT√âGRATION (XX/4)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Tests d'int√©gration]
   Nombre de tests : XX
   Couverture Infrastructure : XX%

4. ASSERTIONS (XX/3)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Qualit√© des assertions]
   Assertions sp√©cifiques : XX%
   Tests edge cases : XX

5. FIXTURES (XX/3)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Organisation et fixtures]
   Fixtures r√©utilisables : XX
   Tests param√©tr√©s : XX

6. PERFORMANCE (XX/2)
   ‚úÖ/‚ö†Ô∏è/‚ùå [Performance des tests]
   Temps total : XXs
   Tests >1s : XX

üéØ TOP 3 ACTIONS PRIORITAIRES :
1. [Action la plus critique pour am√©liorer les tests]
2. [Deuxi√®me action prioritaire]
3. [Troisi√®me action prioritaire]
```

## NOTES

- Ex√©cuter pytest avec coverage pour obtenir les m√©triques
- Utiliser Docker pour s'abstraire de l'environnement local
- Identifier les fichiers critiques sans tests
- Proposer des tests manquants pour les fonctionnalit√©s cl√©s
- Sugg√©rer des am√©liorations concr√®tes des tests existants
- Prioriser les tests selon le risque m√©tier
